# web-crawler

plans by phases:

1. get the arguments from cli
2. make sure the number of arguments is valid
3. check the validity of url and the depth in order to start scanning
4. implements writing values from a demo-array to results.json file.
5. consideration of how to approach to it with asynchronous approach
6. impelement scanning for depth 1 and store it in results.json
7. implement the scanning recursivley by given depth
8. if there is time left => check edges use-cases.
9. writing my thoughts about my approach to this exercise - JS way vs Angular way

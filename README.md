# web-crawler

plans by phases:

1. get the arguments from cli
2. make sure the number of argument is valid
3. check the validity of url and the depth in order to start scanning
4. implements writing values from a demo-array to results.json file
5. impelement scanning for depth 1 and store it in file.json
6. implement the scanning recursivley by given depth
7. if there is time left => check edge use cases.
